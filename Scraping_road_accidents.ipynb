{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2e9642",
   "metadata": {},
   "source": [
    "# Webscraping using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-tribune",
   "metadata": {},
   "source": [
    "## Imports and installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05813f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd \n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs # this is the library that facilitates scraping in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-recognition",
   "metadata": {},
   "source": [
    "Here you can find the [Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e973777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-colonial",
   "metadata": {},
   "source": [
    "Important tool for examining the site structure: Google Chrome Developer Tools. You can access them inside Chrome with \"ctrl + shift + i\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-demonstration",
   "metadata": {},
   "source": [
    "## Setting up the link structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42140bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.thedailystar.net/tags/road-accident\"\n",
    "base_url = \"https://www.thedailystar.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-welcome",
   "metadata": {},
   "source": [
    "## Disclaimer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-guard",
   "metadata": {},
   "source": [
    "Webscraping collects data from websites in an automated fashion. Each request puts an additional load on the server. Running scrapers can put massive loads on servers and bring them down. Most websites do not want to be scraped. The rules for scraping a website can be found in the *robots.txt* site.  \n",
    "Be careful when scraping social media sites, because scraping their content is against their user agreement. You risk to be banned from the social media site and you can get into severe legal trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-security",
   "metadata": {},
   "source": [
    "Let's look at the rules for scraping at our desired website:   [https://www.thedailystar.net/robots.txt](https://www.thedailystar.net/robots.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-calcium",
   "metadata": {},
   "source": [
    "## Trying to request the page\n",
    "If we get a <Response [200]> we are good to go. Otherwise the URL is not reachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = requests.get(url)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(doc) == \"<Response [200]>\":\n",
    "    # create a soup object that contains the navigable html presentation of the page\n",
    "    soup = bs(doc.content, 'html.parser')\n",
    "    print(f\"Retrieved url: {url}\")\n",
    "else:\n",
    "    print(f\"{url} cannot be reached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together into a function\n",
    "def make_soup(url):\n",
    "    doc = requests.get(url)\n",
    "    if str(doc) == \"<Response [200]>\":\n",
    "        # create a soup object that contains the navigable html presentation of the page\n",
    "        soup = bs(doc.content, 'html.parser')\n",
    "        print(f\"Retrieved url: {url}\")\n",
    "    else:\n",
    "        print(f\"{url} cannot be reached.\")\n",
    "        \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-office",
   "metadata": {},
   "source": [
    "## EDA for webscraping\n",
    "Explore the soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-institute",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-holmes",
   "metadata": {},
   "source": [
    "Wow... that is a lot of text. Do we have to find the information with regex?  \n",
    "\"soup\" is NOT a text object but a \"navigable\" object. Let us explore the different ways to navigate to the information that we are looking for.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-diagram",
   "metadata": {},
   "source": [
    "## Knowing HTML syntax\n",
    "It is good to have a basic understanding of the html syntax and how webpages are structured.  \n",
    "\n",
    "**Important tags in a website are:**  \n",
    "h1 - header 1  \n",
    "h2 - header 2  \n",
    "h3 - header 3  \n",
    "h4 - header 4  \n",
    "p - paragraph  \n",
    "div - division  \n",
    "ol - ordered list  \n",
    "ul - unordered list  \n",
    "li - list item  \n",
    "a - link    \n",
    "img - image\n",
    "\n",
    "**Important attributes:**  \n",
    "id - specifies the id for a unique HTML element  \n",
    "class - specifies the class of several HTML elements for attaching CSS code  \n",
    "href - attribute of a link, that indicates the link's destination  \n",
    "src - attribute for the source of an image  \n",
    "\n",
    "Good resource for learning HTML: [https://www.w3schools.com/html/](https://www.w3schools.com/html/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-reference",
   "metadata": {},
   "source": [
    "## Common tasks for Beautiful Soup:\n",
    "Getting all links from a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all links from a page\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all text from a page\n",
    "\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-confirmation",
   "metadata": {},
   "source": [
    "So we just grabbing everything also grabs a lot of whitespace and a lot of duplicate content. We need a strategy that is more specific on selecting only the parts that are relevant for our search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-cassette",
   "metadata": {},
   "source": [
    "## Introducing tags, find and find_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tags\n",
    "tag = soup.li # is just getting the first occurrence of <li> tag\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-ivory",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-acceptance",
   "metadata": {},
   "source": [
    "In order to find the tags inside the soup we can use soup.find() or soup.find_all().  \n",
    "* soup.find() only returns the first object\n",
    "* soup.find_all() returns a lists of all found objects\n",
    "\n",
    "**If you get stuck in drilling down in the soup object, you are most likely trying to call methods on results that were returned as a list. You have to loop over the elements in the list to continue to navigate the soup object.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-pantyhose",
   "metadata": {},
   "source": [
    "##  Fishing in the soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div') # use len() to find out how many items you have found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-volunteer",
   "metadata": {},
   "source": [
    "# Scraping accidents from \"The Daily Star\" \n",
    "## Finding the relevant div inside our webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-courtesy",
   "metadata": {},
   "source": [
    "We want to find the interesting parts on [www.thedailystar.net/tags/road-accident](https://www.thedailystar.net/tags/road-accident)  \n",
    "\n",
    "\n",
    "Use Chrome Developer Tools to narrow down the div that contains all the information that we are interested in. \n",
    "\n",
    "class name: \"view-sub-category-news-listing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-demand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accessing divs that are specified by a class name\n",
    "container = soup.find(\"div\", attrs={\"class\": \"view-sub-category-news-listing\"})\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-senate",
   "metadata": {},
   "source": [
    "In order to get an overview of the structure of the single elements we only look at the first element, to explore it further. find_all returns a list of objects, so we can access the elements by indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-mustang",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "item = container.find_all('li')[0]\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "item.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.find_all('a')[0].attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "container.find(\"h4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(container.find_all(\"h4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "headings = []\n",
    "for row in container.find_all('h4'):\n",
    "    # getting the heading\n",
    "    heading = row.text\n",
    "    headings.append(heading)\n",
    "    \n",
    "    # getting the link to the article\n",
    "    link = row.find('a')\n",
    "    if 'href' in link.attrs:\n",
    "        print(f\"{heading} - {link.attrs['href']}\")\n",
    "        links.append(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-dover",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(container.find_all('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-saskatchewan",
   "metadata": {},
   "source": [
    "## Pagination\n",
    "How can we navigate to the next page?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-guess",
   "metadata": {},
   "source": [
    "### Finding text on the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_button = soup.find(text=\"SHOW MORE\")\n",
    "next_button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-wright",
   "metadata": {},
   "source": [
    "This did not work! Take a look at the webpage in the developer tools and find out why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_button = soup.find(text=\"Show more\")\n",
    "next_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_button.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_button_link = soup.find(text=\"Show more\").parent.attrs['href']\n",
    "next_button_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-equation",
   "metadata": {},
   "source": [
    "The fact that the next page is accessed by a page number can be used to automatically create the link for the next page! Pagination starts at page 0 for the first page.  \n",
    "Let us try to go to the third page: [https://www.thedailystar.net/tags/road-accident?page=2](https://www.thedailystar.net/tags/road-accident?page=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-reward",
   "metadata": {},
   "source": [
    "## Extracting the main article page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-shadow",
   "metadata": {},
   "source": [
    "As we can see this is just the internal link structure. In order to get the complete url we have to construct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_link = base_url+ links[0]\n",
    "page_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup = make_soup(page_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = page_soup.find(\"div\", attrs={\"class\": \"pane-top\"})\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.find('div', attrs={\"class\": \"small-text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_string = top.find('div', attrs={\"class\": \"small-text\"}).text\n",
    "date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = top.find('h1').text\n",
    "headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "author = page_soup.find(\"div\", attrs={\"class\": \"author-name\"}).span.text\n",
    "author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = page_soup.find('div', attrs={\"class\": \"field-body\"})\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = article.find_all('p')\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "subheading = paragraphs[0].text\n",
    "subheading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"\"\n",
    "for i, paragraph in enumerate(paragraphs):\n",
    "    if i == 0:\n",
    "        #print(paragraph.text)\n",
    "        subheading = paragraph\n",
    "    else:\n",
    "        article_text += paragraph.text\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-identification",
   "metadata": {},
   "source": [
    "## TODO: Put it all together\n",
    "1. grab all the links from the first page\n",
    "2. navigate to the next page \n",
    "3. repeat step 1. and 2. until you have gathered all the article links\n",
    "4. grab all the required content from each article page and save it in an appropriate format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-recycling",
   "metadata": {},
   "source": [
    "Keep in mind:  \n",
    "- scrapers tend to fail, so use a lot of try: except: statements\n",
    "- scrape slowly (like a human) or you might get blocked from the website\n",
    "- do not unnecessarily hit the website, grab the page once and then extract all the content. Iterative coding in jupyter notebooks really helps for scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-uncertainty",
   "metadata": {},
   "source": [
    "# Additional resources:\n",
    "\n",
    "Book: [Web Scraping with Python - Oreilly](https://www.amazon.de/Web-Scraping-Python-Collecting-Modern/dp/1491985577) - absolutely worth it!\n",
    "\n",
    "Scraping website with javascript requires the use of Selenium: https://python.gotrained.com/selenium-scraping-booking-com/ \n",
    "\n",
    "Scraping frame work [scrapy](https://scrapy.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-ensemble",
   "metadata": {},
   "source": [
    "# Scraping without understanding content\n",
    "## Trying to scrape text in bangla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "bangla_soup = make_soup('https://www.prothomalo.com/topic/%E0%A6%B8%E0%A7%9C%E0%A6%95-%E0%A6%A6%E0%A7%81%E0%A6%B0%E0%A7%8D%E0%A6%98%E0%A6%9F%E0%A6%A8%E0%A6%BE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = bangla_soup.find_all(\"div\", attrs={\"class\": \"bn-story-card\"})\n",
    "stories[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = stories[0].find(\"div\", attrs={'data-testid': 'tag-related'}).find('time').text\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "for story in stories:\n",
    "    print(story.find(\"div\", attrs={'data-testid': 'tag-related'}).find('time').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-functionality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
